{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **using both L1 and L2 regularization**"
      ],
      "metadata": {
        "id": "clGb21cF6zfA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzP469-t51B5",
        "outputId": "9f8f137e-ac29-4fa4-8aaa-32987d758a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 1.7547\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.5934\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.5515\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.5153\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4611\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.4158\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.3794\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.3392\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.3008\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.2649\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a05c546fca0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "# Sample data: Let's assume X_train and y_train are your training data.\n",
        "# For example purposes, we are creating random data.\n",
        "import numpy as np\n",
        "X_train = np.random.random((100, 10))\n",
        "y_train = np.random.random((100, 1))\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    # Add L1 regularization to the first layer\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,),\n",
        "                 kernel_regularizer=regularizers.l1(0.01)),  # L1 regularization\n",
        "    # Add L2 regularization to the second layer\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),  # L2 regularization\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dropout and Monte Carlo Dropout in a neural network using TensorFlow and Keras:**"
      ],
      "metadata": {
        "id": "dadZdRFZ7Ac-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "X_train = np.random.random((100, 10))\n",
        "y_train = np.random.random((100, 1))\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "\n",
        "def monte_carlo_predictions(model, X, n_samples=100):\n",
        "    predictions = [model.predict(X) for _ in range(n_samples)]\n",
        "    return np.mean(predictions, axis=0), np.std(predictions, axis=0)\n",
        "\n",
        "\n",
        "mean_predictions, std_predictions = monte_carlo_predictions(model, X_train)\n",
        "\n",
        "print(\"Mean predictions:\", mean_predictions)\n",
        "print(\"Standard deviation of predictions:\", std_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM1S3b3a7Czt",
        "outputId": "71595751-a6ec-41d4-ec7a-f5646ba262c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 1s 7ms/step - loss: 0.7619\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4475\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2946\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2311\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2021\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1965\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1756\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1438\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1582\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1624\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Mean predictions: [[0.5986669 ]\n",
            " [0.45800784]\n",
            " [0.48175916]\n",
            " [0.4908612 ]\n",
            " [0.52151877]\n",
            " [0.50493234]\n",
            " [0.5300102 ]\n",
            " [0.50361365]\n",
            " [0.46395877]\n",
            " [0.5347281 ]\n",
            " [0.43582404]\n",
            " [0.49137616]\n",
            " [0.34560642]\n",
            " [0.4200871 ]\n",
            " [0.5598041 ]\n",
            " [0.29591954]\n",
            " [0.61332315]\n",
            " [0.44098145]\n",
            " [0.48874637]\n",
            " [0.54415125]\n",
            " [0.34990147]\n",
            " [0.40898272]\n",
            " [0.45263425]\n",
            " [0.6134608 ]\n",
            " [0.48731583]\n",
            " [0.65528196]\n",
            " [0.4151587 ]\n",
            " [0.4848926 ]\n",
            " [0.4496893 ]\n",
            " [0.44235265]\n",
            " [0.4947004 ]\n",
            " [0.5883445 ]\n",
            " [0.4784711 ]\n",
            " [0.67570025]\n",
            " [0.4768321 ]\n",
            " [0.38326877]\n",
            " [0.5677798 ]\n",
            " [0.3175012 ]\n",
            " [0.527116  ]\n",
            " [0.43389022]\n",
            " [0.548827  ]\n",
            " [0.6458588 ]\n",
            " [0.5242463 ]\n",
            " [0.5299607 ]\n",
            " [0.56935126]\n",
            " [0.46095413]\n",
            " [0.57264304]\n",
            " [0.6131813 ]\n",
            " [0.34143662]\n",
            " [0.46944907]\n",
            " [0.5647965 ]\n",
            " [0.43501246]\n",
            " [0.6584272 ]\n",
            " [0.4966917 ]\n",
            " [0.6222155 ]\n",
            " [0.46610644]\n",
            " [0.61337286]\n",
            " [0.5415753 ]\n",
            " [0.5167095 ]\n",
            " [0.36325705]\n",
            " [0.7224307 ]\n",
            " [0.5542028 ]\n",
            " [0.44554523]\n",
            " [0.5821838 ]\n",
            " [0.61829394]\n",
            " [0.5590705 ]\n",
            " [0.4131204 ]\n",
            " [0.5392198 ]\n",
            " [0.55091774]\n",
            " [0.46698654]\n",
            " [0.37286606]\n",
            " [0.41770515]\n",
            " [0.2956968 ]\n",
            " [0.38667136]\n",
            " [0.55183405]\n",
            " [0.39767691]\n",
            " [0.54646295]\n",
            " [0.37543622]\n",
            " [0.6036977 ]\n",
            " [0.4314256 ]\n",
            " [0.5956039 ]\n",
            " [0.455601  ]\n",
            " [0.49659085]\n",
            " [0.48659635]\n",
            " [0.5015374 ]\n",
            " [0.56484324]\n",
            " [0.63055706]\n",
            " [0.37043393]\n",
            " [0.42490384]\n",
            " [0.48593017]\n",
            " [0.65633875]\n",
            " [0.4893684 ]\n",
            " [0.39138797]\n",
            " [0.44626614]\n",
            " [0.46783864]\n",
            " [0.54918677]\n",
            " [0.60636115]\n",
            " [0.3445719 ]\n",
            " [0.45775497]\n",
            " [0.29102948]]\n",
            "Standard deviation of predictions: [[5.9604645e-07]\n",
            " [1.1920929e-07]\n",
            " [3.5762787e-07]\n",
            " [4.4703484e-07]\n",
            " [1.1920929e-07]\n",
            " [5.9604645e-08]\n",
            " [0.0000000e+00]\n",
            " [3.5762787e-07]\n",
            " [2.0861626e-07]\n",
            " [1.7881393e-07]\n",
            " [1.4901161e-07]\n",
            " [4.1723251e-07]\n",
            " [3.5762787e-07]\n",
            " [1.1920929e-07]\n",
            " [5.9604645e-07]\n",
            " [1.4901161e-07]\n",
            " [1.7881393e-07]\n",
            " [4.1723251e-07]\n",
            " [4.1723251e-07]\n",
            " [4.1723251e-07]\n",
            " [1.7881393e-07]\n",
            " [1.4901161e-07]\n",
            " [4.1723251e-07]\n",
            " [4.7683716e-07]\n",
            " [6.5565109e-07]\n",
            " [8.9406967e-07]\n",
            " [5.9604645e-08]\n",
            " [2.3841858e-07]\n",
            " [5.0663948e-07]\n",
            " [5.0663948e-07]\n",
            " [3.8743019e-07]\n",
            " [5.9604645e-08]\n",
            " [5.9604645e-07]\n",
            " [2.9802322e-07]\n",
            " [4.7683716e-07]\n",
            " [0.0000000e+00]\n",
            " [7.1525574e-07]\n",
            " [3.5762787e-07]\n",
            " [5.3644180e-07]\n",
            " [8.9406967e-08]\n",
            " [7.7486038e-07]\n",
            " [5.9604645e-08]\n",
            " [1.1920929e-07]\n",
            " [1.7881393e-07]\n",
            " [0.0000000e+00]\n",
            " [5.3644180e-07]\n",
            " [7.1525574e-07]\n",
            " [5.9604645e-07]\n",
            " [1.7881393e-07]\n",
            " [1.1920929e-07]\n",
            " [2.9802322e-07]\n",
            " [3.5762787e-07]\n",
            " [5.3644180e-07]\n",
            " [2.3841858e-07]\n",
            " [4.7683716e-07]\n",
            " [1.7881393e-07]\n",
            " [1.1920929e-07]\n",
            " [5.3644180e-07]\n",
            " [5.9604645e-07]\n",
            " [1.1920929e-07]\n",
            " [4.7683716e-07]\n",
            " [4.1723251e-07]\n",
            " [1.1920929e-07]\n",
            " [4.1723251e-07]\n",
            " [4.7683716e-07]\n",
            " [4.1723251e-07]\n",
            " [3.2782555e-07]\n",
            " [1.7881393e-07]\n",
            " [3.5762787e-07]\n",
            " [2.9802322e-07]\n",
            " [0.0000000e+00]\n",
            " [5.0663948e-07]\n",
            " [4.1723251e-07]\n",
            " [2.9802322e-07]\n",
            " [5.9604645e-08]\n",
            " [1.4901161e-07]\n",
            " [1.7881393e-07]\n",
            " [2.9802322e-08]\n",
            " [6.5565109e-07]\n",
            " [3.2782555e-07]\n",
            " [1.1920929e-07]\n",
            " [3.2782555e-07]\n",
            " [4.1723251e-07]\n",
            " [2.3841858e-07]\n",
            " [2.9802322e-07]\n",
            " [5.9604645e-07]\n",
            " [8.9406967e-07]\n",
            " [2.3841858e-07]\n",
            " [2.9802322e-08]\n",
            " [5.0663948e-07]\n",
            " [7.7486038e-07]\n",
            " [0.0000000e+00]\n",
            " [2.0861626e-07]\n",
            " [2.9802322e-08]\n",
            " [5.9604645e-07]\n",
            " [2.9802322e-07]\n",
            " [5.3644180e-07]\n",
            " [5.9604645e-08]\n",
            " [2.6822090e-07]\n",
            " [3.8743019e-07]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Regularization**"
      ],
      "metadata": {
        "id": "oDjkwqey7QBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, regularizers\n",
        "from keras import backend as K\n",
        "\n",
        "# Define a custom regularization class\n",
        "class FrobeniusRegularizer(regularizers.Regularizer):\n",
        "    def __init__(self, strength):\n",
        "        self.strength = strength\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Penalize the Frobenius norm of the weights matrix\n",
        "        return self.strength * K.sqrt(K.sum(K.square(x)))\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'strength': self.strength}\n",
        "\n",
        "# Sample data: Let's assume X_train and y_train are your training data.\n",
        "# For example purposes, we are creating random data.\n",
        "import numpy as np\n",
        "X_train = np.random.random((100, 10))\n",
        "y_train = np.random.random((100, 1))\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,),\n",
        "                 kernel_regularizer=FrobeniusRegularizer(strength=0.01)),  # Apply custom regularization\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSLxM6BI7WEi",
        "outputId": "a050d2c1-d67d-45f1-a2d9-2a7e5b55ea69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 0.3529\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1853\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1315\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1348\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1345\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1251\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1192\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1176\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1157\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1136\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a05c5229120>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **early stopping in a TensorFlow and Keras model**"
      ],
      "metadata": {
        "id": "z1S6QvZk7jt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import numpy as np\n",
        "X_train = np.random.random((100, 10))\n",
        "y_train = np.random.random((100, 1))\n",
        "X_val = np.random.random((20, 10))\n",
        "y_val = np.random.random((20, 1))\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Configure early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor the validation loss\n",
        "    patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,           # Log a message when stopping\n",
        "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "# Train the model with early stopping\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # The maximum number of epochs to run\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping]  # Include early stopping in the callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBqdQQq7pzE",
        "outputId": "2bed8a4a-e450-4cfc-841c-58ff0b947364"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 71ms/step - loss: 0.1281 - val_loss: 0.0854\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0917 - val_loss: 0.0881\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1006 - val_loss: 0.0814\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0870 - val_loss: 0.0817\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0802 - val_loss: 0.0832\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0787 - val_loss: 0.0837\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0772 - val_loss: 0.0851\n",
            "Epoch 8/100\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0901Restoring model weights from the end of the best epoch: 3.\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0758 - val_loss: 0.0848\n",
            "Epoch 8: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a05c53d0df0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Batch Normalization**"
      ],
      "metadata": {
        "id": "tZ40X9Ia7u6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "# Sample data: Let's assume X_train and y_train are your training data.\n",
        "# For example purposes, we are creating random data.\n",
        "import numpy as np\n",
        "X_train = np.random.random((100, 10))\n",
        "y_train = np.random.random((100, 1))\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "    layers.BatchNormalization(),  # Batch Normalization layer after the first dense layer\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.BatchNormalization(),  # Another Batch Normalization layer\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1jg4D9M70vS",
        "outputId": "f401d4ec-cf30-44ab-c6cc-889f2c7b70a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 1.6461\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0377\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8473\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5887\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5237\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4572\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3525\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3373\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2468\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a05c4cfb1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Various Initializations**"
      ],
      "metadata": {
        "id": "Xhk7_Ofg79kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.initializers import GlorotNormal, HeNormal\n",
        "\n",
        "# Sample data: Let's assume X_train and y_train are your training data.\n",
        "# For example purposes, we are creating random data.\n",
        "import numpy as np\n",
        "X_train = np.random.random((100, 10))\n",
        "y_train = np.random.random((100, 1))\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    # Layer with Glorot (Xavier) Normal Initialization\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,),\n",
        "                 kernel_initializer=GlorotNormal()),\n",
        "    # Layer with He Normal Initialization\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_initializer=HeNormal()),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Vpgw0T78Z_",
        "outputId": "2a73d718-ac35-4137-a8da-4717eddd287f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 0.6369\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2998\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1288\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0785\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1059\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1213\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1033\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0839\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0745\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0746\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a05c55993c0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}