{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Model Architectures and Training Techniques**"
      ],
      "metadata": {
        "id": "MhY5ctNt3luU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Gaussian Noise Layer**"
      ],
      "metadata": {
        "id": "_cOT1RWN4LOF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW58C_4y3TTS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "class AddGaussianNoise(keras.layers.Layer):\n",
        "    def __init__(self, mean=0.0, stddev=0.1):\n",
        "        super(AddGaussianNoise, self).__init__()\n",
        "        self.mean = mean\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            noise = tf.random.normal(shape=tf.shape(inputs), mean=self.mean, stddev=self.stddev)\n",
        "            return inputs + noise\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Model Including the Noise Layer**"
      ],
      "metadata": {
        "id": "NH1JlRtI46yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize data\n",
        "x_train = x_train[..., np.newaxis]\n",
        "x_test = x_test[..., np.newaxis]\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.Input(shape=(28, 28, 1)),\n",
        "    AddGaussianNoise(stddev=0.2),  # Customize stddev as needed\n",
        "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAuxvKBK5GNc",
        "outputId": "8ebc013b-727e-4f78-821d-0e8b666e856e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 37s 48ms/step - loss: 0.5317 - accuracy: 0.8084 - val_loss: 0.4608 - val_accuracy: 0.8496\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.3926 - accuracy: 0.8564 - val_loss: 0.3985 - val_accuracy: 0.8739\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 30s 40ms/step - loss: 0.3583 - accuracy: 0.8689 - val_loss: 0.4088 - val_accuracy: 0.8685\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 30s 41ms/step - loss: 0.3377 - accuracy: 0.8734 - val_loss: 0.3680 - val_accuracy: 0.8781\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 29s 39ms/step - loss: 0.3188 - accuracy: 0.8808 - val_loss: 0.3453 - val_accuracy: 0.8843\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 29s 39ms/step - loss: 0.3067 - accuracy: 0.8851 - val_loss: 0.3569 - val_accuracy: 0.8825\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.2978 - accuracy: 0.8891 - val_loss: 0.3480 - val_accuracy: 0.8782\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 30s 40ms/step - loss: 0.2879 - accuracy: 0.8907 - val_loss: 0.3348 - val_accuracy: 0.8829\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.2749 - accuracy: 0.8966 - val_loss: 0.3175 - val_accuracy: 0.8905\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.2661 - accuracy: 0.8986 - val_loss: 0.3375 - val_accuracy: 0.8811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmM6Spsz6qth",
        "outputId": "9852b84c-d674-40d8-a665-e813af0bcb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3510 - accuracy: 0.8754\n",
            "Test Accuracy: 0.8754000067710876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Residual Block**"
      ],
      "metadata": {
        "id": "8ryUnZJU7qCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, num_filters):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = keras.layers.Conv2D(num_filters, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn1 = keras.layers.BatchNormalization()\n",
        "        self.conv2 = keras.layers.Conv2D(num_filters, kernel_size=3, padding='same', activation=None)\n",
        "        self.bn2 = keras.layers.BatchNormalization()\n",
        "        self.relu = keras.layers.Activation('relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x += inputs  # Adding the input to the output of the block\n",
        "        x = self.relu(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Cl2kE_-w7s_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Using the Residual Block**"
      ],
      "metadata": {
        "id": "MvEhCCgr70BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.Input(shape=(28, 28, 1)),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same'),\n",
        "    ResidualBlock(32),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    ResidualBlock(32),\n",
        "    keras.layers.GlobalAveragePooling2D(),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhYgVQsD76b8",
        "outputId": "b4b2d329-9d3d-4724-c981-fade112450f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 275s 364ms/step - loss: 0.6748 - accuracy: 0.7895 - val_loss: 0.5769 - val_accuracy: 0.7939\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 274s 365ms/step - loss: 0.3671 - accuracy: 0.8756 - val_loss: 0.5685 - val_accuracy: 0.7814\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 266s 355ms/step - loss: 0.3119 - accuracy: 0.8913 - val_loss: 0.4115 - val_accuracy: 0.8540\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 278s 371ms/step - loss: 0.2836 - accuracy: 0.8989 - val_loss: 0.3276 - val_accuracy: 0.8854\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 266s 355ms/step - loss: 0.2593 - accuracy: 0.9078 - val_loss: 0.3286 - val_accuracy: 0.8823\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 265s 354ms/step - loss: 0.2468 - accuracy: 0.9128 - val_loss: 0.3388 - val_accuracy: 0.8801\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 260s 346ms/step - loss: 0.2333 - accuracy: 0.9163 - val_loss: 0.4751 - val_accuracy: 0.8369\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 262s 349ms/step - loss: 0.2236 - accuracy: 0.9208 - val_loss: 0.4912 - val_accuracy: 0.8457\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 268s 357ms/step - loss: 0.2158 - accuracy: 0.9238 - val_loss: 0.4764 - val_accuracy: 0.8335\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 268s 357ms/step - loss: 0.2048 - accuracy: 0.9282 - val_loss: 0.2988 - val_accuracy: 0.8932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the Model**"
      ],
      "metadata": {
        "id": "nAzRyRbV8YDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjy2IeB38Myt",
        "outputId": "f1268f84-05e8-453f-f92d-780c3e89f2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 13s 42ms/step - loss: 0.3156 - accuracy: 0.8933\n",
            "Test Accuracy: 0.8932999968528748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AddGaussianNoise Layer**"
      ],
      "metadata": {
        "id": "_3VjPUnF_CSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddGaussianNoise(keras.layers.Layer):\n",
        "    def __init__(self, mean=0.0, stddev=0.1):\n",
        "        super(AddGaussianNoise, self).__init__()\n",
        "        self.mean = mean\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            noise = tf.random.normal(shape=tf.shape(inputs), mean=self.mean, stddev=self.stddev)\n",
        "            return inputs + noise\n",
        "        return inputs\n"
      ],
      "metadata": {
        "id": "zGLw6brF_FUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Using the AddGaussianNoise Layer**"
      ],
      "metadata": {
        "id": "N000RhQx_KQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Fashion MNIST data\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Reshape data\n",
        "x_train = x_train[..., np.newaxis]\n",
        "x_test = x_test[..., np.newaxis]\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    AddGaussianNoise(0.0, 0.1),\n",
        "    keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRfeAZC1_NM0",
        "outputId": "ec5d88e4-fd02-450b-8534-3e039d873537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 8s 9ms/step - loss: 0.6214 - accuracy: 0.7800 - val_loss: 0.4328 - val_accuracy: 0.8419\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 8s 10ms/step - loss: 0.4690 - accuracy: 0.8314 - val_loss: 0.3971 - val_accuracy: 0.8578\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 6s 8ms/step - loss: 0.4356 - accuracy: 0.8408 - val_loss: 0.3883 - val_accuracy: 0.8557\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 7s 9ms/step - loss: 0.4116 - accuracy: 0.8501 - val_loss: 0.3744 - val_accuracy: 0.8632\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 6s 7ms/step - loss: 0.3970 - accuracy: 0.8549 - val_loss: 0.3609 - val_accuracy: 0.8722\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 7s 9ms/step - loss: 0.3848 - accuracy: 0.8603 - val_loss: 0.3600 - val_accuracy: 0.8673\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 6s 8ms/step - loss: 0.3821 - accuracy: 0.8619 - val_loss: 0.3534 - val_accuracy: 0.8700\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 7s 9ms/step - loss: 0.3702 - accuracy: 0.8618 - val_loss: 0.3348 - val_accuracy: 0.8784\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 6s 8ms/step - loss: 0.3656 - accuracy: 0.8648 - val_loss: 0.3348 - val_accuracy: 0.8790\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 7s 9ms/step - loss: 0.3567 - accuracy: 0.8673 - val_loss: 0.3556 - val_accuracy: 0.8722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the Model**"
      ],
      "metadata": {
        "id": "s-0SSe-R_QYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWD4wmgE_StI",
        "outputId": "ebb98a96-eabd-4b4e-a534-546edfcd004e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3845 - accuracy: 0.8640\n",
            "Test Accuracy: 0.8640000224113464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **custom training loop**"
      ],
      "metadata": {
        "id": "G9dSz5OK_lCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize and reshape data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.LayerNormalization(),\n",
        "    keras.layers.Dense(10)  # No activation function as logits will be output\n",
        "])\n",
        "\n",
        "# Although using a custom loop, still need to compile for the evaluation\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Custom training loop\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            loss_value = keras.losses.sparse_categorical_crossentropy(y_batch_train, logits, from_logits=True)\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(\"Training loss (for one batch) at step %d: %.4f\" % (step, float(loss_value.numpy().mean())))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AuxVDdLJhsa",
        "outputId": "8c6365b5-4d1c-40a1-b424-0bff8c4a761b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 2.8375\n",
            "Training loss (for one batch) at step 200: 0.4221\n",
            "Training loss (for one batch) at step 400: 0.2709\n",
            "Training loss (for one batch) at step 600: 0.3404\n",
            "Training loss (for one batch) at step 800: 0.5491\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.3692\n",
            "Training loss (for one batch) at step 200: 0.2799\n",
            "Training loss (for one batch) at step 400: 0.5351\n",
            "Training loss (for one batch) at step 600: 0.3936\n",
            "Training loss (for one batch) at step 800: 0.4826\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 0.1773\n",
            "Training loss (for one batch) at step 200: 0.2271\n",
            "Training loss (for one batch) at step 400: 0.2805\n",
            "Training loss (for one batch) at step 600: 0.3208\n",
            "Training loss (for one batch) at step 800: 0.2764\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 0.2591\n",
            "Training loss (for one batch) at step 200: 0.4343\n",
            "Training loss (for one batch) at step 400: 0.3098\n",
            "Training loss (for one batch) at step 600: 0.2442\n",
            "Training loss (for one batch) at step 800: 0.1644\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 0.2257\n",
            "Training loss (for one batch) at step 200: 0.3111\n",
            "Training loss (for one batch) at step 400: 0.3248\n",
            "Training loss (for one batch) at step 600: 0.1906\n",
            "Training loss (for one batch) at step 800: 0.3721\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 0.2929\n",
            "Training loss (for one batch) at step 200: 0.2799\n",
            "Training loss (for one batch) at step 400: 0.1985\n",
            "Training loss (for one batch) at step 600: 0.2553\n",
            "Training loss (for one batch) at step 800: 0.3364\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 0.1937\n",
            "Training loss (for one batch) at step 200: 0.1038\n",
            "Training loss (for one batch) at step 400: 0.1967\n",
            "Training loss (for one batch) at step 600: 0.1061\n",
            "Training loss (for one batch) at step 800: 0.2352\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 0.1828\n",
            "Training loss (for one batch) at step 200: 0.3236\n",
            "Training loss (for one batch) at step 400: 0.2174\n",
            "Training loss (for one batch) at step 600: 0.2866\n",
            "Training loss (for one batch) at step 800: 0.3377\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 0.3192\n",
            "Training loss (for one batch) at step 200: 0.1368\n",
            "Training loss (for one batch) at step 400: 0.3098\n",
            "Training loss (for one batch) at step 600: 0.1376\n",
            "Training loss (for one batch) at step 800: 0.3120\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 0.1879\n",
            "Training loss (for one batch) at step 200: 0.2203\n",
            "Training loss (for one batch) at step 400: 0.2150\n",
            "Training loss (for one batch) at step 600: 0.2327\n",
            "Training loss (for one batch) at step 800: 0.1608\n",
            "313/313 - 1s - loss: 0.3270 - accuracy: 0.8871 - 897ms/epoch - 3ms/step\n",
            "Test loss: 0.3269578218460083\n",
            "Test accuracy: 0.8870999813079834\n"
          ]
        }
      ]
    }
  ]
}