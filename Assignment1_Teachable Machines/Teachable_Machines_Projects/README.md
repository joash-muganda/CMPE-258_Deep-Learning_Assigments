# Multimodal Interaction Project

Welcome to the Multimodal Interaction Project repository! This project showcases a collection of machine learning models developed using Teachable Machine. These models are designed to recognize and interpret various forms of human interaction, including American Sign Language (ASL), sounds from African musical instruments, and distinct body poses. Created as a part of an academic course, this project aims to explore and demonstrate the potential of machine learning in understanding diverse human modalities.

## Project Overview

This project is structured into three primary components, each focusing on a different interaction modality:

1. **ASL Alphabet Recognition**: Interprets the handshapes of the ASL alphabet.
2. **African Musical Instruments Sound Recognition**: Identifies sounds from traditional African musical instruments.
3. **Body Pose Recognition**: Recognizes specific body poses associated with various actions.

## Detailed Components

### ASL Alphabet Recognition

- **Model URL**: [Access the ASL Model here](https://teachablemachine.withgoogle.com/models/Brw7mJxkw/)
- **Functionality**: Capable of recognizing the 26 letters of the ASL alphabet through handshapes.

### African Musical Instruments Sound Recognition

- **Model URL**: [Access the African Instruments Model here](https://teachablemachine.withgoogle.com/models/R4ybKNMn3/)
- **Instruments Covered**:
  - Orutu: A single-stringed fiddle known for its unique sound.
  - Nyatiti: A stringed instrument resembling a lyre, integral to Luo music.
  - Ohangla: Traditional drums that produce a vibrant rhythmic beat.
- **Capabilities**: This model distinguishes the distinct sounds produced by the Orutu, Nyatiti, and Ohangla, celebrating Africa's rich musical heritage.

### Body Pose Recognition

- **Model URL**: [Access the Body Pose Model here](https://teachablemachine.withgoogle.com/models/FWcZxPsc0/)
- **Poses Recognized**: Includes actions like Throwing, Jumping, Kicking, Ducking, Powering Up, Punching, and Standing.
- **Overview**: Trained to identify seven unique body poses, this model facilitates interactive human-machine interfaces through physical gestures.

## Video Demonstrations

Explore the models in action through our video demonstrations:

- **Pose Project Demo**: [View the demonstration](https://drive.google.com/file/d/1OY6L9pmZRft8iQtoUFJl_YMyHn7zpmly/view?usp=sharing) to see the Body Pose Recognition model at work.
- **Image Project Demo**: [View the demonstration](https://drive.google.com/file/d/1pcwI11eIq30nQKyTuRWyuHXe7ePYoa6t/view?usp=sharing) for insights into the ASL Alphabet and African Musical Instruments Sound Recognition models.
- **Audio Project Demo**: [View the demonstration](https://drive.google.com/file/d/1GsGX5g4V5nafjguvTvJGXh87-v2NWA0M/view?usp=sharing) showcasing the capabilities of the African Musical Instruments Sound Recognition model.

These videos provide a comprehensive view of how each model interprets and responds to different modalities, demonstrating the practical applications of the project.


## Usage Instructions

To interact with the models, visit the provided model URLs. You'll find interfaces to test the ASL and Body Pose Recognition models in real-time using your webcam. For the African Musical Instruments Sound Recognition model, there's an option to upload or directly record sounds for analysis.

