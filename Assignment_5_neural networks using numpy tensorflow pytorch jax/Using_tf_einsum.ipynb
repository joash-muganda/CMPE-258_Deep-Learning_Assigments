{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV3T4wjvbd5F",
        "outputId": "da471848-f206-4187-b282-051e15871e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 2.000098466873169\n",
            "Epoch 2: Loss = 1.810802936553955\n",
            "Epoch 3: Loss = 1.6838812828063965\n",
            "Epoch 4: Loss = 1.6823647022247314\n",
            "Epoch 5: Loss = 1.6806391477584839\n",
            "Epoch 6: Loss = 1.6806840896606445\n",
            "Epoch 7: Loss = 1.6785045862197876\n",
            "Epoch 8: Loss = 1.6839649677276611\n",
            "Epoch 9: Loss = 1.6810803413391113\n",
            "Epoch 10: Loss = 1.680355191230774\n",
            "Epoch 11: Loss = 1.6780260801315308\n",
            "Epoch 12: Loss = 1.680052638053894\n",
            "Epoch 13: Loss = 1.6775925159454346\n",
            "Epoch 14: Loss = 1.6790844202041626\n",
            "Epoch 15: Loss = 1.684868335723877\n",
            "Epoch 16: Loss = 1.679939866065979\n",
            "Epoch 17: Loss = 1.6825802326202393\n",
            "Epoch 18: Loss = 1.6765129566192627\n",
            "Epoch 19: Loss = 1.6792603731155396\n",
            "Epoch 20: Loss = 1.6859278678894043\n",
            "Epoch 21: Loss = 1.6778944730758667\n",
            "Epoch 22: Loss = 1.6779855489730835\n",
            "Epoch 23: Loss = 1.6791094541549683\n",
            "Epoch 24: Loss = 1.6802124977111816\n",
            "Epoch 25: Loss = 1.6766886711120605\n",
            "Epoch 26: Loss = 1.6782385110855103\n",
            "Epoch 27: Loss = 1.677445650100708\n",
            "Epoch 28: Loss = 1.6780738830566406\n",
            "Epoch 29: Loss = 1.6788896322250366\n",
            "Epoch 30: Loss = 1.6797730922698975\n",
            "Epoch 31: Loss = 1.680485486984253\n",
            "Epoch 32: Loss = 1.6792597770690918\n",
            "Epoch 33: Loss = 1.6787570714950562\n",
            "Epoch 34: Loss = 1.678800106048584\n",
            "Epoch 35: Loss = 1.685233473777771\n",
            "Epoch 36: Loss = 1.6806318759918213\n",
            "Epoch 37: Loss = 1.6793667078018188\n",
            "Epoch 38: Loss = 1.6807650327682495\n",
            "Epoch 39: Loss = 1.6798310279846191\n",
            "Epoch 40: Loss = 1.6761668920516968\n",
            "Epoch 41: Loss = 1.6787009239196777\n",
            "Epoch 42: Loss = 1.6794180870056152\n",
            "Epoch 43: Loss = 1.6824990510940552\n",
            "Epoch 44: Loss = 1.6791355609893799\n",
            "Epoch 45: Loss = 1.6786282062530518\n",
            "Epoch 46: Loss = 1.6783562898635864\n",
            "Epoch 47: Loss = 1.6800082921981812\n",
            "Epoch 48: Loss = 1.6781615018844604\n",
            "Epoch 49: Loss = 1.676888108253479\n",
            "Epoch 50: Loss = 1.6822938919067383\n",
            "Epoch 51: Loss = 1.6791729927062988\n",
            "Epoch 52: Loss = 1.6776831150054932\n",
            "Epoch 53: Loss = 1.6774934530258179\n",
            "Epoch 54: Loss = 1.6777681112289429\n",
            "Epoch 55: Loss = 1.6798266172409058\n",
            "Epoch 56: Loss = 1.6796108484268188\n",
            "Epoch 57: Loss = 1.677391767501831\n",
            "Epoch 58: Loss = 1.678633451461792\n",
            "Epoch 59: Loss = 1.6777756214141846\n",
            "Epoch 60: Loss = 1.6789227724075317\n",
            "Epoch 61: Loss = 1.6787445545196533\n",
            "Epoch 62: Loss = 1.6783298254013062\n",
            "Epoch 63: Loss = 1.680543065071106\n",
            "Epoch 64: Loss = 1.677965521812439\n",
            "Epoch 65: Loss = 1.6783185005187988\n",
            "Epoch 66: Loss = 1.676966905593872\n",
            "Epoch 67: Loss = 1.6770802736282349\n",
            "Epoch 68: Loss = 1.6793574094772339\n",
            "Epoch 69: Loss = 1.6823147535324097\n",
            "Epoch 70: Loss = 1.6746188402175903\n",
            "Epoch 71: Loss = 1.6771938800811768\n",
            "Epoch 72: Loss = 1.676530122756958\n",
            "Epoch 73: Loss = 1.681017279624939\n",
            "Epoch 74: Loss = 1.6770787239074707\n",
            "Epoch 75: Loss = 1.6800987720489502\n",
            "Epoch 76: Loss = 1.6796284914016724\n",
            "Epoch 77: Loss = 1.6783658266067505\n",
            "Epoch 78: Loss = 1.6770941019058228\n",
            "Epoch 79: Loss = 1.6824309825897217\n",
            "Epoch 80: Loss = 1.6788440942764282\n",
            "Epoch 81: Loss = 1.6798617839813232\n",
            "Epoch 82: Loss = 1.6771965026855469\n",
            "Epoch 83: Loss = 1.6773481369018555\n",
            "Epoch 84: Loss = 1.6806544065475464\n",
            "Epoch 85: Loss = 1.6787136793136597\n",
            "Epoch 86: Loss = 1.6777452230453491\n",
            "Epoch 87: Loss = 1.6774286031723022\n",
            "Epoch 88: Loss = 1.680019497871399\n",
            "Epoch 89: Loss = 1.6790698766708374\n",
            "Epoch 90: Loss = 1.6779121160507202\n",
            "Epoch 91: Loss = 1.6775903701782227\n",
            "Epoch 92: Loss = 1.683061957359314\n",
            "Epoch 93: Loss = 1.683867335319519\n",
            "Epoch 94: Loss = 1.6777070760726929\n",
            "Epoch 95: Loss = 1.6781688928604126\n",
            "Epoch 96: Loss = 1.6826449632644653\n",
            "Epoch 97: Loss = 1.681138515472412\n",
            "Epoch 98: Loss = 1.6782214641571045\n",
            "Epoch 99: Loss = 1.682753086090088\n",
            "Epoch 100: Loss = 1.6786538362503052\n",
            "Test Loss: 1.6410795450210571\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class ThreeLayerNN(tf.Module):\n",
        "    def __init__(self, input_size=3, hidden_size=64, output_size=1):\n",
        "        super().__init__()\n",
        "        self.W1 = tf.Variable(tf.random.normal([input_size, hidden_size], stddev=0.01))\n",
        "        self.b1 = tf.Variable(tf.zeros([hidden_size]))\n",
        "        self.W2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], stddev=0.01))\n",
        "        self.b2 = tf.Variable(tf.zeros([hidden_size]))\n",
        "        self.W3 = tf.Variable(tf.random.normal([hidden_size, output_size], stddev=0.01))\n",
        "        self.b3 = tf.Variable(tf.zeros([output_size]))\n",
        "\n",
        "    def __call__(self, X):\n",
        "        Z1 = tf.einsum('ij,jk->ik', X, self.W1) + self.b1\n",
        "        A1 = tf.nn.relu(Z1)\n",
        "        Z2 = tf.einsum('ij,jk->ik', A1, self.W2) + self.b2\n",
        "        A2 = tf.nn.relu(Z2)\n",
        "        Z3 = tf.einsum('ij,jk->ik', A2, self.W3) + self.b3\n",
        "        return Z3\n",
        "\n",
        "\n",
        "def generate_data(num_samples=1000):\n",
        "    np.random.seed(42)\n",
        "    x = np.random.uniform(-2, 2, num_samples)\n",
        "    y = np.random.uniform(-2, 2, num_samples)\n",
        "    z = np.random.uniform(-2, 2, num_samples)\n",
        "    f_xyz = np.sin(x) + np.cos(y) * np.power(z, 2)\n",
        "    features = np.vstack((x, y, z)).T\n",
        "    outputs = f_xyz\n",
        "    return features, outputs\n",
        "\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "# Generate and prepare data\n",
        "features, outputs = generate_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, outputs, test_size=0.2, random_state=42)\n",
        "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = ThreeLayerNN()\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Define training step\n",
        "@tf.function\n",
        "def train_step(X, y, model, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X)\n",
        "        loss = compute_loss(y, y_pred)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "def train(model, optimizer, X_train_tf, y_train_tf, epochs=100, batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf)).shuffle(buffer_size=1024).batch(batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "        for X_batch, y_batch in dataset:\n",
        "            loss = train_step(X_batch, y_batch, model, optimizer)\n",
        "            epoch_loss_avg.update_state(loss)\n",
        "        print(f\"Epoch {epoch + 1}: Loss = {epoch_loss_avg.result().numpy()}\")\n",
        "\n",
        "# Train the model\n",
        "train(model, optimizer, X_train_tf, y_train_tf)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model(X_test_tf)\n",
        "test_loss = compute_loss(y_test_tf, y_pred)\n",
        "print(f\"Test Loss: {test_loss.numpy()}\")\n"
      ]
    }
  ]
}