{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_iwOMPnVp7M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_data(num_samples=1000):\n",
        "    x = np.random.uniform(-2, 2, num_samples)\n",
        "    y = np.random.uniform(-2, 2, num_samples)\n",
        "    z = np.random.uniform(-2, 2, num_samples)\n",
        "    f_xyz = np.sin(x) + np.cos(y) * np.power(z, 2)\n",
        "    features = np.vstack((x, y, z)).T\n",
        "    outputs = f_xyz\n",
        "    return features, outputs\n",
        "\n",
        "features, outputs = generate_data()\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, outputs, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "# Neural network parameters\n",
        "input_size = 3\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "\n",
        "# Weights and biases\n",
        "W1 = tf.Variable(tf.random.normal([input_size, hidden_size], stddev=0.01), name='W1')\n",
        "b1 = tf.Variable(tf.zeros([hidden_size]), name='b1')\n",
        "W2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], stddev=0.01), name='W2')\n",
        "b2 = tf.Variable(tf.zeros([hidden_size]), name='b2')\n",
        "W3 = tf.Variable(tf.random.normal([hidden_size, output_size], stddev=0.01), name='W3')\n",
        "b3 = tf.Variable(tf.zeros([output_size]), name='b3')\n",
        "\n",
        "def forward_pass(X):\n",
        "    \"\"\"\n",
        "    Performs the forward pass of the neural network.\n",
        "    \"\"\"\n",
        "    Z1 = tf.add(tf.matmul(X, W1), b1)\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(A1, W2), b2)\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(A2, W3), b3)\n",
        "    return Z3\n",
        "\n",
        "def compute_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes the Mean Squared Error.\n",
        "    \"\"\"\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "@tf.function\n",
        "def train_step(X, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = forward_pass(X)\n",
        "        loss = compute_loss(y, y_pred)\n",
        "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "vPOXsK9GYcIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "num_batches = int(X_train_tf.shape[0] / batch_size)\n",
        "\n",
        "# Convert training data to TensorFlow Dataset for easy batching\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "    for X_batch, y_batch in train_dataset:\n",
        "        loss = train_step(X_batch, y_batch)\n",
        "        epoch_loss_avg.update_state(loss)\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch + 1}: Loss = {epoch_loss_avg.result().numpy()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = forward_pass(X_test_tf)\n",
        "test_loss = compute_loss(y_test_tf, y_pred)\n",
        "print(f\"Test Loss: {test_loss.numpy()}\")\n"
      ],
      "metadata": {
        "id": "QJ6xfGZDYigU",
        "outputId": "e69fdfaa-0b55-485f-dbda-ee267ca5bcec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 2.006441116333008\n",
            "Epoch 2: Loss = 1.8089503049850464\n",
            "Epoch 3: Loss = 1.6864248514175415\n",
            "Epoch 4: Loss = 1.6811546087265015\n",
            "Epoch 5: Loss = 1.6835821866989136\n",
            "Epoch 6: Loss = 1.681503415107727\n",
            "Epoch 7: Loss = 1.6818797588348389\n",
            "Epoch 8: Loss = 1.6809359788894653\n",
            "Epoch 9: Loss = 1.6829965114593506\n",
            "Epoch 10: Loss = 1.6815180778503418\n",
            "Epoch 11: Loss = 1.6833759546279907\n",
            "Epoch 12: Loss = 1.6844717264175415\n",
            "Epoch 13: Loss = 1.6818554401397705\n",
            "Epoch 14: Loss = 1.681178331375122\n",
            "Epoch 15: Loss = 1.6794716119766235\n",
            "Epoch 16: Loss = 1.6804394721984863\n",
            "Epoch 17: Loss = 1.677107334136963\n",
            "Epoch 18: Loss = 1.679297924041748\n",
            "Epoch 19: Loss = 1.6771857738494873\n",
            "Epoch 20: Loss = 1.6805959939956665\n",
            "Epoch 21: Loss = 1.6833518743515015\n",
            "Epoch 22: Loss = 1.6808096170425415\n",
            "Epoch 23: Loss = 1.6762219667434692\n",
            "Epoch 24: Loss = 1.6780892610549927\n",
            "Epoch 25: Loss = 1.679823398590088\n",
            "Epoch 26: Loss = 1.6820461750030518\n",
            "Epoch 27: Loss = 1.6796519756317139\n",
            "Epoch 28: Loss = 1.6797932386398315\n",
            "Epoch 29: Loss = 1.6793274879455566\n",
            "Epoch 30: Loss = 1.6829321384429932\n",
            "Epoch 31: Loss = 1.6777288913726807\n",
            "Epoch 32: Loss = 1.6785649061203003\n",
            "Epoch 33: Loss = 1.6785694360733032\n",
            "Epoch 34: Loss = 1.6772068738937378\n",
            "Epoch 35: Loss = 1.6771184206008911\n",
            "Epoch 36: Loss = 1.682183861732483\n",
            "Epoch 37: Loss = 1.6800625324249268\n",
            "Epoch 38: Loss = 1.682918667793274\n",
            "Epoch 39: Loss = 1.6879355907440186\n",
            "Epoch 40: Loss = 1.6844415664672852\n",
            "Epoch 41: Loss = 1.6755331754684448\n",
            "Epoch 42: Loss = 1.6819671392440796\n",
            "Epoch 43: Loss = 1.6792510747909546\n",
            "Epoch 44: Loss = 1.6799747943878174\n",
            "Epoch 45: Loss = 1.6776307821273804\n",
            "Epoch 46: Loss = 1.682654857635498\n",
            "Epoch 47: Loss = 1.681282639503479\n",
            "Epoch 48: Loss = 1.6772630214691162\n",
            "Epoch 49: Loss = 1.681374192237854\n",
            "Epoch 50: Loss = 1.6841354370117188\n",
            "Epoch 51: Loss = 1.6808186769485474\n",
            "Epoch 52: Loss = 1.6778390407562256\n",
            "Epoch 53: Loss = 1.6799426078796387\n",
            "Epoch 54: Loss = 1.6790276765823364\n",
            "Epoch 55: Loss = 1.6805323362350464\n",
            "Epoch 56: Loss = 1.6786296367645264\n",
            "Epoch 57: Loss = 1.6815733909606934\n",
            "Epoch 58: Loss = 1.6796045303344727\n",
            "Epoch 59: Loss = 1.6779818534851074\n",
            "Epoch 60: Loss = 1.6775201559066772\n",
            "Epoch 61: Loss = 1.678553581237793\n",
            "Epoch 62: Loss = 1.679114580154419\n",
            "Epoch 63: Loss = 1.680722951889038\n",
            "Epoch 64: Loss = 1.6805238723754883\n",
            "Epoch 65: Loss = 1.6813971996307373\n",
            "Epoch 66: Loss = 1.6885404586791992\n",
            "Epoch 67: Loss = 1.681584119796753\n",
            "Epoch 68: Loss = 1.680004596710205\n",
            "Epoch 69: Loss = 1.6788439750671387\n",
            "Epoch 70: Loss = 1.6801315546035767\n",
            "Epoch 71: Loss = 1.677708387374878\n",
            "Epoch 72: Loss = 1.6791315078735352\n",
            "Epoch 73: Loss = 1.678390622138977\n",
            "Epoch 74: Loss = 1.6770251989364624\n",
            "Epoch 75: Loss = 1.6771950721740723\n",
            "Epoch 76: Loss = 1.6771506071090698\n",
            "Epoch 77: Loss = 1.6768821477890015\n",
            "Epoch 78: Loss = 1.6799688339233398\n",
            "Epoch 79: Loss = 1.680113673210144\n",
            "Epoch 80: Loss = 1.6792269945144653\n",
            "Epoch 81: Loss = 1.6769698858261108\n",
            "Epoch 82: Loss = 1.6769747734069824\n",
            "Epoch 83: Loss = 1.6783925294876099\n",
            "Epoch 84: Loss = 1.6792281866073608\n",
            "Epoch 85: Loss = 1.6756813526153564\n",
            "Epoch 86: Loss = 1.6776375770568848\n",
            "Epoch 87: Loss = 1.6786723136901855\n",
            "Epoch 88: Loss = 1.6809144020080566\n",
            "Epoch 89: Loss = 1.6759411096572876\n",
            "Epoch 90: Loss = 1.678518533706665\n",
            "Epoch 91: Loss = 1.676414966583252\n",
            "Epoch 92: Loss = 1.678892970085144\n",
            "Epoch 93: Loss = 1.6773697137832642\n",
            "Epoch 94: Loss = 1.677189588546753\n",
            "Epoch 95: Loss = 1.6769163608551025\n",
            "Epoch 96: Loss = 1.6791448593139648\n",
            "Epoch 97: Loss = 1.6784805059432983\n",
            "Epoch 98: Loss = 1.6785156726837158\n",
            "Epoch 99: Loss = 1.6773467063903809\n",
            "Epoch 100: Loss = 1.6784050464630127\n",
            "Test Loss: 1.6413531303405762\n"
          ]
        }
      ]
    }
  ]
}